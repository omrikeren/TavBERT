{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import multiset\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "from seqeval.scheme import IOBES, IOB2\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "from fairseq.models.roberta import RobertaModel\n",
    "\n",
    "def parse_file_line(line, with_word_counts=False):\n",
    "    if line.startswith('VOID'):\n",
    "        token = ' '\n",
    "        label = 'VOID'\n",
    "        wc = '-'\n",
    "    else:\n",
    "        if with_word_counts:\n",
    "            token, label, wc = line.split()\n",
    "        else:\n",
    "            token, label = line.split()\n",
    "    if with_word_counts:\n",
    "        return label, token, wc\n",
    "    return label, token\n",
    "\n",
    "\n",
    "def majority_vote(preds):\n",
    "    value, _ = Counter(preds).most_common()[0]\n",
    "    return value\n",
    "\n",
    "\n",
    "def convert_segmented_preds_to_multitags(preds, use_majority_vote=False):\n",
    "    \"\"\"\n",
    "    Concatenates all unique tags in the given sequence until reaching the first tag that exists in the rest of the word\n",
    "    (including this tag - henceforth, the recurring tag). If use_majority_vote is True, then all preceding tags are\n",
    "    combined with the most common tag in the rest of the word. Otherwise, we concatenate the recurring tag.\n",
    "    E.g. for the following tag sequence: t_1, t_2, t_3, t_4, t_5, t_3, t_3, t_60, t_100,\n",
    "    the resulting multi-tag prediction will be: t_1+t_2+t_3\n",
    "    \"\"\"\n",
    "    unique_tags = []\n",
    "\n",
    "    for ind, p in enumerate(preds):\n",
    "        if p in preds[ind + 1:]:\n",
    "            if not use_majority_vote:\n",
    "                if p not in unique_tags:\n",
    "                    unique_tags.append(p)\n",
    "                break\n",
    "            c = Counter(preds[ind:])\n",
    "            value, _ = c.most_common()[0]\n",
    "            if p not in unique_tags:\n",
    "                unique_tags.append(value)\n",
    "            break\n",
    "        if p not in unique_tags:\n",
    "            unique_tags.append(p)\n",
    "\n",
    "    return '+'.join(list(unique_tags))\n",
    "\n",
    "\n",
    "def convert_by_pred_spans(preds):\n",
    "    unique_tags = []\n",
    "    i = 0\n",
    "    while i < len(preds):\n",
    "        curr_pred = preds[i]\n",
    "        unique_tags.append(curr_pred)\n",
    "        try:\n",
    "            last_occurrence = len(preds) - preds[::-1].index(curr_pred) - 1\n",
    "        except ValueError:\n",
    "            i += 1\n",
    "        else:\n",
    "            i = last_occurrence + 1\n",
    "    return '+'.join(unique_tags)\n",
    "\n",
    "\n",
    "def split_by_spaces_and_punct(sent, labels, sent_delims=['%'], tokenizer=None, ud_format=True):\n",
    "    if tokenizer is not None:\n",
    "        return get_original_words_from_tokens(sent, tokenizer)\n",
    "\n",
    "    start = 0\n",
    "    spans = []\n",
    "    word_tokens = []\n",
    "    for ind, (ch, l) in enumerate(zip(sent, labels)):\n",
    "        if l == 'VOID':\n",
    "            if start != ind:\n",
    "                spans.append((start, ind))\n",
    "            start = ind + 1\n",
    "            word_tokens = []\n",
    "        elif (ud_format and l == 'PUNCT') or (not ud_format and l.startswith(\"yy\")) or ch in sent_delims:\n",
    "            if start != ind:\n",
    "                spans.append((start, ind))\n",
    "            spans.append((ind, ind + 1))\n",
    "            start = ind + 1\n",
    "            word_tokens = []\n",
    "        else:\n",
    "            word_tokens.append(ch)\n",
    "    if start < len(sent):\n",
    "        spans.append((start, len(sent)))\n",
    "    return spans\n",
    "\n",
    "\n",
    "def get_original_words_from_tokens(sent, tokenizer):\n",
    "    toks = tokenizer(sent, add_special_tokens=False, return_offsets_mapping=True)[0]\n",
    "    previous_word_id = None\n",
    "    word_offsets = []\n",
    "    start, end = None, None\n",
    "    for word_id, offset in zip(toks.word_ids, toks.offsets):\n",
    "        if word_id != previous_word_id:\n",
    "            if (start, end) != (None, None):\n",
    "                word_offsets.append((start, end))\n",
    "            start, end = offset\n",
    "        else:\n",
    "            end = offset[1]\n",
    "        previous_word_id = word_id\n",
    "    word_offsets.append((start, end))\n",
    "    return word_offsets\n",
    "\n",
    "\n",
    "def gather_predictions_from_subwords(pos_preds, word_spans):\n",
    "    \"\"\"\n",
    "    Returns a list of POS predictions determined by the first token of each word in word_spans.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    spans_c, pos_pred_c = 0, 0\n",
    "    while pos_pred_c < len(pos_preds) and spans_c < len(word_spans):\n",
    "        pos_pred = pos_preds[pos_pred_c]\n",
    "        if (pos_pred['start'], pos_pred['end']) == word_spans[spans_c]:\n",
    "            res.append(pos_pred['entity'])\n",
    "            spans_c += 1\n",
    "            pos_pred_c += 1\n",
    "        elif pos_pred['start'] == word_spans[spans_c][0]:\n",
    "            e = pos_pred['entity']\n",
    "            pos_pred_c += 1\n",
    "        elif pos_pred['end'] == word_spans[spans_c][1]:\n",
    "            res.append(e)\n",
    "            spans_c += 1\n",
    "            pos_pred_c += 1\n",
    "        elif e == 'PUNCT':\n",
    "            res.append(e)\n",
    "            spans_c += 1\n",
    "        else:\n",
    "            pos_pred_c += 1\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_predictions_from_huggingface_output(pred_file):\n",
    "    preds = []\n",
    "    with open(pred_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            preds.append(line.split(' '))\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_spans(wcs):\n",
    "    spans = []\n",
    "    inds = [i for i in np.unique(wcs) if i.isdigit()]\n",
    "    for i in inds:\n",
    "        t = np.where(np.array(wcs) == i)[0]\n",
    "        spans.append((min(t), max(t) + 1))\n",
    "    return sorted(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5), (6, 11), (12, 14), (15, 20), (21, 25), (26, 30), (31, 38), (38, 39)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['VERB',\n",
       " 'ADP+NOUN+ADP+PRON',\n",
       " 'ADP',\n",
       " 'NOUN',\n",
       " 'DET+NOUN',\n",
       " 'ADP+DET+NOUN',\n",
       " 'DET+ADJ',\n",
       " 'PUNCT']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [{'word': 'הצג',\n",
    "  'score': 0.9994872212409973,\n",
    "  'entity': 'VERB',\n",
    "  'index': 1,\n",
    "  'start': 0,\n",
    "  'end': 3},\n",
    " {'word': '##נו',\n",
    "  'score': 0.9989884495735168,\n",
    "  'entity': 'VERB',\n",
    "  'index': 2,\n",
    "  'start': 3,\n",
    "  'end': 5},\n",
    " {'word': 'בפניה',\n",
    "  'score': 0.9901555180549622,\n",
    "  'entity': 'ADP+NOUN+ADP+PRON',\n",
    "  'index': 3,\n",
    "  'start': 6,\n",
    "  'end': 11},\n",
    " {'word': 'את',\n",
    "  'score': 0.9995697736740112,\n",
    "  'entity': 'ADP',\n",
    "  'index': 4,\n",
    "  'start': 12,\n",
    "  'end': 14},\n",
    " {'word': 'רעיון',\n",
    "  'score': 0.9996181130409241,\n",
    "  'entity': 'NOUN',\n",
    "  'index': 5,\n",
    "  'start': 15,\n",
    "  'end': 20},\n",
    " {'word': 'הכפ',\n",
    "  'score': 0.998690664768219,\n",
    "  'entity': 'DET+NOUN',\n",
    "  'index': 6,\n",
    "  'start': 21,\n",
    "  'end': 24},\n",
    " {'word': '##ל',\n",
    "  'score': 0.9961203932762146,\n",
    "  'entity': 'DET+NOUN',\n",
    "  'index': 7,\n",
    "  'start': 24,\n",
    "  'end': 25},\n",
    " {'word': 'בפעם',\n",
    "  'score': 0.9948251843452454,\n",
    "  'entity': 'ADP+DET+NOUN',\n",
    "  'index': 8,\n",
    "  'start': 26,\n",
    "  'end': 30},\n",
    " {'word': 'הראשונה',\n",
    "  'score': 0.9980702996253967,\n",
    "  'entity': 'DET+ADJ',\n",
    "  'index': 9,\n",
    "  'start': 31,\n",
    "  'end': 38},\n",
    " {'word': '.',\n",
    "  'score': 0.9997915029525757,\n",
    "  'entity': 'PUNCT',\n",
    "  'index': 10,\n",
    "  'start': 38,\n",
    "  'end': 39}]\n",
    "\n",
    "sent = 'הצגנו בפניה את רעיון הכפל בפעם הראשונה.'\n",
    "labels = ['VERB'] * 5 + ['VOID'] + ['ADP+PRON'] * 5 + ['VOID'] + \\\n",
    "['ADP'] * 2 + ['VOID'] + ['NOUN'] * 5 + ['VOID'] + ['DET+NOUN'] * 4 + ['VOID'] + ['ADP+DET+NOUN'] * 4 + \\\n",
    "['VOID'] + ['DET+ADJ'] * 7 + ['PUNCT']\n",
    "word_spans = split_by_spaces_and_punct(sent, labels)\n",
    "print(word_spans)\n",
    "gather_predictions_from_subwords(preds, word_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_morph_tags_file(gold_dir, feats):\n",
    "    tagged_sents = []\n",
    "    sent = []\n",
    "    for i, f in enumerate(feats):\n",
    "        with open(os.path.join(gold_dir, f'{f}_test_segmented.txt'), 'r', encoding='utf-8') as fobj:\n",
    "            sent_id = 0\n",
    "            for line in fobj:\n",
    "                line = line.strip()\n",
    "                if line.isdigit():\n",
    "                    continue\n",
    "                if line == '':\n",
    "                    if i == 0:\n",
    "                        tagged_sents.append([(t, (l,)) for t, l in sent])\n",
    "                    else:\n",
    "                        tagged_sents[sent_id] = [(t, l + (sent[j][1],)) for j, (t, l) in\n",
    "                                                 enumerate(tagged_sents[sent_id])]\n",
    "                    sent = []\n",
    "                    sent_id += 1\n",
    "                    continue\n",
    "                if line == 'VOID' or line == 'X':\n",
    "                    tok = ' '\n",
    "                    tag = line\n",
    "                else:\n",
    "                    tok, tag = line.split()\n",
    "                sent.append((tok, tag))\n",
    "\n",
    "    return tagged_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligned_f1_score(fn_total, fp_total, tp_total):\n",
    "    precision = tp_total / (tp_total + fp_total)\n",
    "    recall = tp_total / (tp_total + fn_total)\n",
    "    aligned_mset_f1 = 2 * precision * recall / (precision + recall)\n",
    "    aligned_mset_f1 = 100 * aligned_mset_f1\n",
    "    return aligned_mset_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_multiset_f1(checkpoint_path=None, huggingface_checkpoint=False, predictions_path=None, split=\"dev\",\n",
    "                     pred_format=\"segmented\", gold_format=\"segmented\", use_majority_vote=False, use_prediction_spans=False,\n",
    "                     use_data_from_dir=True, output_file=None, eval_morph=True, gold_data_dir=None, ud_format=True,\n",
    "                    verbose=False):\n",
    "    \"\"\"The main function for evaluating mset-F1 scores for POS and morphological features.\"\"\"\n",
    "    feats = ['abbr',\n",
    "             'case',\n",
    "             'definite',\n",
    "             'gender',\n",
    "             'hebbinyan',\n",
    "             'hebexistential',\n",
    "             'hebsource',\n",
    "             'mood',\n",
    "             'number',\n",
    "             'person',\n",
    "             'polarity',\n",
    "             'prefix',\n",
    "             'prontype',\n",
    "             'reflex',\n",
    "             'tense',\n",
    "             'verbform',\n",
    "             'verbtype',\n",
    "             'voice',\n",
    "             'xtra'] if ud_format else ['feats_gen',\n",
    "                                        'feats_HebBinyan',\n",
    "                                        'feats_num',\n",
    "                                        'feats_per',\n",
    "                                        'feats_polar',\n",
    "                                        'feats_suf_gen',\n",
    "                                        'feats_suf_num',\n",
    "                                        'feats_suf_per',\n",
    "                                        'feats_tense']\n",
    "\n",
    "    if checkpoint_path is not None:\n",
    "        print(\"Loading model from checkpoint {} ...\".format(checkpoint_path))\n",
    "    if huggingface_checkpoint:\n",
    "        id2label = {int(i): l for i, l in\n",
    "                    json.load(open(os.path.join(str(Path(checkpoint_path).parent), 'ids_to_labels.json'), 'r')).items()}\n",
    "        label2id = {l: i for i, l in id2label.items()}\n",
    "        config = AutoConfig.from_pretrained(str(Path(checkpoint_path).parent), label2id=label2id, id2label=id2label)\n",
    "        model = AutoModelForTokenClassification.from_pretrained(str(Path(checkpoint_path).parent), config=config)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(str(Path(checkpoint_path).parent))\n",
    "        roberta = pipeline(task='ner', model=model, tokenizer=tokenizer)\n",
    "    elif predictions_path is None:\n",
    "        roberta = RobertaModel.from_pretrained(str(Path(checkpoint_path).parent),\n",
    "                                               checkpoint_file=os.path.basename(checkpoint_path),\n",
    "                                               override_data_key=use_data_from_dir)\n",
    "        roberta.eval()\n",
    "    else:\n",
    "        predictions = get_predictions_from_huggingface_output(predictions_path)\n",
    "\n",
    "    tokenizer = None\n",
    "    if gold_data_dir is None:\n",
    "        gold_data_dir = str(Path(checkpoint_path).parent)\n",
    "\n",
    "    gold_pos_path = os.path.join(gold_data_dir, \"pos_{}_{}{}.txt\".format(split, gold_format, \n",
    "                                                                         \"_spmrl\" if not ud_format else \"_ud\"))\n",
    "    if eval_morph:\n",
    "        morph_tagged_sents = read_from_morph_tags_file(gold_data_dir, feats=feats, split=split)\n",
    "        sent_lens = [len(s) for s in morph_tagged_sents]\n",
    "        \n",
    "    print(\"Evaluating msetF1 for {} results in path:\".format(split), gold_pos_path)\n",
    "    tp_total, fp_total, fn_total = {}, {}, {}\n",
    "    eval_sets = ['pos', 'morph', 'all'] if eval_morph else ['pos']\n",
    "    for d in [tp_total, fp_total, fn_total]:\n",
    "        for eval_set in eval_sets:\n",
    "            d[eval_set] = 0\n",
    "\n",
    "    error_stats = []\n",
    "    multitag_sep_char = '+'\n",
    "    with open(gold_pos_path, 'r', encoding='utf-8') as fin_pos:\n",
    "        sent = ''\n",
    "        sent_id = 0\n",
    "        sent_count = 0\n",
    "        token_ind_in_sent = 0\n",
    "        gold_pos = []\n",
    "        n_errors = 0\n",
    "        n_words = 0\n",
    "        for pos_line in tqdm(fin_pos):\n",
    "            pos_line = pos_line.strip()\n",
    "            try:\n",
    "                sent_id = int(pos_line)\n",
    "                continue\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "            if pos_line != '':\n",
    "                try:\n",
    "                    parse_file_line(pos_line)\n",
    "                except ValueError:\n",
    "                    # New sentence\n",
    "                    sent_id = pos_line\n",
    "                    continue\n",
    "                else:\n",
    "                    pos_label, token = parse_file_line(pos_line)\n",
    "                    token_ind_in_sent += 1\n",
    "                    sent += token\n",
    "                    gold_pos.append(pos_label)\n",
    "                    continue\n",
    "\n",
    "            if huggingface_checkpoint:\n",
    "                pos_preds = roberta(sent)\n",
    "            elif predictions_path is None:\n",
    "                tokens = roberta.encode(sent)\n",
    "                if eval_morph:\n",
    "                    pos_preds = roberta.predict_tags('pos' if ud_format else 'upostag', tokens)\n",
    "                    morph_preds = list(zip(*[roberta.predict_tags(feat, tokens) for feat in feats]))\n",
    "                    assert len(pos_preds) == sent_lens[\n",
    "                        sent_count], f\"Error! Length of POS preds ({len(pos_preds)}) does not match length of \" \\\n",
    "                                     f\"morph preds ({sent_lens[sent_count]}).\"\n",
    "                    assert len(morph_preds) == sent_lens[\n",
    "                        sent_count], f\"Error! Length of POS preds ({len(pos_preds)}) does not match length of \" \\\n",
    "                                     f\"morph preds ({sent_lens[sent_count]}).\"\n",
    "                else:\n",
    "                    pos_preds = roberta.predict_tags('postagging', tokens)\n",
    "\n",
    "            word_spans = split_by_spaces_and_punct(sent, gold_pos, tokenizer=tokenizer, ud_format=ud_format,\n",
    "                                                   sent_delims=['%', ':', ';'])\n",
    "            n_words += len(word_spans)\n",
    "            if huggingface_checkpoint:\n",
    "                pos_preds = gather_predictions_from_subwords(pos_preds, word_spans, majority=use_majority_vote)\n",
    "            elif predictions_path is not None:\n",
    "                pos_preds = predictions[sent_count]\n",
    "\n",
    "            if huggingface_checkpoint:\n",
    "                assert len(word_spans) == len(pos_preds), f\"Error! Number of word spans: {len(word_spans)}, \" \\\n",
    "                                                          f\"number of POS predictions: {len(pos_preds)}, \" \\\n",
    "                                                          f\"sentence is: {sent}\"\n",
    "            for ind, (w_start, w_end) in enumerate(word_spans):\n",
    "                for g in [gold_pos]:\n",
    "                    assert \"VOID\" not in g[w_start:w_end], \\\n",
    "                        \"Error! Found a VOID tag in gold labels for the word {}\".format(sent[w_start:w_end])\n",
    "\n",
    "                if pred_format == \"segmented\" and gold_format == \"multitag\":\n",
    "                    # Convert the input format into the multi-tag format by splitting all united tags\n",
    "                    if w_end - w_start == 1:\n",
    "                        union_pos_pred = pos_preds[w_start:w_end][0]\n",
    "                    elif use_prediction_spans:\n",
    "                        union_pos_pred = convert_by_pred_spans(pos_preds[w_start:w_end])\n",
    "                    else:\n",
    "                        union_pos_pred = convert_segmented_preds_to_multitags(pos_preds[w_start:w_end],\n",
    "                                                                              use_majority_vote=use_majority_vote)\n",
    "                    pos_pred_tokens = union_pos_pred.split(multitag_sep_char)\n",
    "                else:  # multitag -> multitag\n",
    "                    if w_end - w_start == 1:\n",
    "                        if predictions_path is not None:\n",
    "                            pos_pred_tokens = [pos_preds[ind]]\n",
    "                        elif not huggingface_checkpoint:\n",
    "                            pos_pred_tokens = pos_preds[w_start:w_end]\n",
    "                        elif huggingface_checkpoint:\n",
    "                            pos_pred_tokens = [pos_preds[ind]]\n",
    "                    elif use_prediction_spans:\n",
    "                        union_pos_pred = convert_by_pred_spans(pos_preds[w_start:w_end])\n",
    "                        pos_pred_tokens = union_pos_pred.split(multitag_sep_char)\n",
    "                    elif use_majority_vote:\n",
    "                        if huggingface_checkpoint:\n",
    "                            pos_pred_tokens = pos_preds[ind].split(multitag_sep_char)\n",
    "                        else:\n",
    "                            union_pos_pred = majority_vote(pos_preds[w_start:w_end])\n",
    "                            pos_pred_tokens = union_pos_pred.split(multitag_sep_char)\n",
    "                    else:  # prediction by first token\n",
    "                        if predictions_path is not None or huggingface_checkpoint:\n",
    "                            pos_pred_tokens = pos_preds[ind].split(multitag_sep_char)\n",
    "                        else:\n",
    "                            pos_pred_tokens = pos_preds[w_start].split(multitag_sep_char)\n",
    "\n",
    "                pos_gold_tokens = convert_by_pred_spans(gold_pos[w_start:w_end]).split(multitag_sep_char)\n",
    "\n",
    "                if eval_morph:\n",
    "                    morph_pred_tokens = []\n",
    "                    morph_gold_tokens = []\n",
    "                    for i in range(len(feats)):\n",
    "                        if use_prediction_spans:\n",
    "                            pred_feat = convert_by_pred_spans([p[i] for p in morph_preds[w_start:w_end]])\n",
    "                            gold_feat = convert_by_pred_spans(\n",
    "                                [p[1][i] for p in morph_tagged_sents[sent_count][w_start:w_end]])\n",
    "                        else:\n",
    "                            pred_feat = majority_vote([p[i] for p in morph_preds[w_start:w_end]])\n",
    "                            gold_feat = majority_vote(\n",
    "                                [p[1][i] for p in morph_tagged_sents[sent_count][w_start:w_end]])\n",
    "                        if pred_feat != 'X':\n",
    "                            pred_feat = [p for p in pred_feat.split(multitag_sep_char) if p != 'X']\n",
    "                        else:\n",
    "                            pred_feat = []\n",
    "\n",
    "                        if gold_feat != 'X':\n",
    "                            gold_feat = [p for p in gold_feat.split(multitag_sep_char) if p != 'X']\n",
    "                        else:\n",
    "                            gold_feat = []\n",
    "\n",
    "                        pred_feat = [p for p in pred_feat if p != '']\n",
    "                        gold_feat = [g for g in gold_feat if g != '']\n",
    "                        morph_pred_tokens.extend(pred_feat)\n",
    "                        morph_gold_tokens.extend(gold_feat)\n",
    "\n",
    "                    all_pred_tokens = pos_pred_tokens + morph_pred_tokens\n",
    "\n",
    "                    for g in [gold_pos]:\n",
    "                        assert len(set(g[w_start:w_end])) == 1, \\\n",
    "                            \"Error! Not all gold lables are identical for the word {}: {}\".format(\n",
    "                                sent[w_start:w_end], g[w_start:w_end])\n",
    "\n",
    "                    all_gold_tokens = pos_gold_tokens + morph_gold_tokens\n",
    "                else:\n",
    "                    all_pred_tokens = pos_pred_tokens\n",
    "                    all_gold_tokens = pos_gold_tokens\n",
    "\n",
    "                if verbose and sorted(pos_pred_tokens) != sorted(pos_gold_tokens):\n",
    "                    n_errors += 1\n",
    "                    print(\"========================\")\n",
    "                    print(\"Wrong prediction in sentence id:\", sent_id)\n",
    "                    print(\"Sentence:\", sent)\n",
    "                    print(\"Word:\", sent[w_start:w_end])\n",
    "                    if huggingface_checkpoint:\n",
    "                        print(\"Pred POS labels:\", pos_preds[ind])\n",
    "                    else:\n",
    "                        print(\"Pred POS labels:\", pos_preds[w_start:w_end])\n",
    "                    print(\"Gold POS labels:\", gold_pos[w_start:w_end])\n",
    "\n",
    "                    error_stats.append({\"sent_id\": sent_id,\n",
    "                                        \"sent_text\": sent,\n",
    "                                        \"word\": sent[w_start:w_end],\n",
    "                                        \"pred_pos\": pos_preds[\n",
    "                                            ind] if huggingface_checkpoint else pos_preds[w_start:w_end],\n",
    "                                        \"gold_pos\": gold_pos[w_start:w_end],\n",
    "                                        \"pred_mset\": all_pred_tokens if eval_morph else pos_pred_tokens,\n",
    "                                        \"gold_mset\": all_gold_tokens if eval_morph else pos_gold_tokens})\n",
    "                    if eval_morph:\n",
    "                        print(\"Separated pred labels (All):\", all_pred_tokens)\n",
    "                        print(\"Separated gold labels (All):\", all_gold_tokens)\n",
    "\n",
    "                    print(\"Separated pred labels (POS):\", pos_pred_tokens)\n",
    "                    print(\"Separated gold labels (POS):\", pos_gold_tokens)\n",
    "\n",
    "                for eval_set in eval_sets:\n",
    "                    if eval_set == 'pos':\n",
    "                        pred_toks = pos_pred_tokens\n",
    "                        gold_toks = pos_gold_tokens\n",
    "                    elif eval_set == 'morph':\n",
    "                        pred_toks = morph_pred_tokens\n",
    "                        gold_toks = morph_gold_tokens\n",
    "                    else:\n",
    "                        pred_toks = all_pred_tokens\n",
    "                        gold_toks = all_gold_tokens\n",
    "                    tp_token = multiset.Multiset(pred_toks).intersection(multiset.Multiset(gold_toks))\n",
    "                    fp_token = multiset.Multiset(pred_toks).difference(multiset.Multiset(gold_toks))\n",
    "                    fn_token = multiset.Multiset(gold_toks).difference(multiset.Multiset(pred_toks))\n",
    "                    tp_total[eval_set] += len(tp_token)\n",
    "                    fp_total[eval_set] += len(fp_token)\n",
    "                    fn_total[eval_set] += len(fn_token)\n",
    "            sent = ''\n",
    "            token_ind_in_sent = 0\n",
    "            gold_pos = []\n",
    "            sent_count += 1\n",
    "\n",
    "    aligned_mset_f1 = {}\n",
    "    for eval_set in eval_sets:\n",
    "        aligned_mset_f1[eval_set] = aligned_f1_score(fn_total[eval_set],\n",
    "                                                     fp_total[eval_set],\n",
    "                                                     tp_total[eval_set])\n",
    "\n",
    "    if output_file is not None:\n",
    "        logging_str = \"============ Results ============\\n\"\n",
    "        logging_str += \"Checkpoint path: {}\\n\".format(checkpoint_path)\n",
    "        logging_str += \"Prediction format: {}\\n\".format(pred_format)\n",
    "        logging_str += \"Gold format: {}\\n\".format(gold_format)\n",
    "        logging_str += \"Heuristics used: Majority={}, Prediction Spans={}\\n\".format(use_majority_vote,\n",
    "                                                                                    use_prediction_spans)\n",
    "        for eval_set in eval_sets:\n",
    "            logging_str += \"Aligned MSET-F1 on {} for {}: {:.2f}\\n\".format(split, eval_set, aligned_mset_f1[eval_set])\n",
    "        open(output_file, 'w').write(logging_str)\n",
    "        open('{}.json'.format(output_file), 'w', encoding='utf-8').write(\n",
    "            json.dumps(error_stats, indent=4, ensure_ascii=False))\n",
    "\n",
    "    return aligned_mset_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calc_multiset_f1(#checkpoint_path=r\"checkpoints\\ud_pos_segmented_new_tavbert_ar_lr_0.0001_bsz_32_5epochs\\checkpoint5.pt\", \n",
    "                 predictions_path=r\"checkpoints\\tavbert_base_he_pos_lr_0.0001_bsz_16_5epochs\\predictions.txt\",\n",
    "                 split=\"test\", \n",
    "                 pred_format=\"segmented\",\n",
    "                 gold_format=\"multitag\", \n",
    "                 use_majority_vote=False, \n",
    "                 use_prediction_spans=True,\n",
    "                 use_data_from_dir=True, \n",
    "                 output_file=\"postagging_only_tavbert_base_he_segmented_new.txt\", \n",
    "                 eval_morph=False, \n",
    "                 gold_data_dir=r\"finetuning\\pos\\data\",\n",
    "                 ud_format=True,\n",
    "                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_ner_f1(checkpoint_path, predictions_path=None, split=\"dev\", use_majority_vote=False,\n",
    "                use_prediction_spans=False,\n",
    "                use_data_from_dir=True, output_file=None, gold_data_dir=None, set_id=1, scheme=IOBES, f1_type=\"micro\",\n",
    "               ner_format='single'):\n",
    "    print(\"Loading model from checkpoint {} ...\".format(checkpoint_path))\n",
    "    if predictions_path is None:\n",
    "        roberta = RobertaModel.from_pretrained(str(Path(checkpoint_path).parent),\n",
    "                                               checkpoint_file=os.path.basename(checkpoint_path),\n",
    "                                               override_data_key=use_data_from_dir)\n",
    "        pprint.pprint(roberta.cfg)\n",
    "        roberta.eval()\n",
    "    else:\n",
    "        predictions = [p.strip().split() for p in open(predictions_path, 'r').readlines()]\n",
    "\n",
    "    if gold_data_dir is None:\n",
    "        gold_data_dir = str(Path(checkpoint_path).parent)\n",
    "\n",
    "    gold_ner_path_wc = os.path.join(gold_data_dir, f\"token-single_gold_{split}.bmes_word_counts\")\n",
    "    print(f\"Evaluating F1 for {split} on set {set_id}, with results in path: {gold_ner_path_wc}\")\n",
    "    tp_total, fp_total, fn_total = 0, 0, 0\n",
    "    y_true, y_pred, preds_tok_label = [], [], []\n",
    "    error_stats = []\n",
    "    gold_words = []\n",
    "    with open(gold_ner_path_wc, 'r', encoding='utf-8') as fin_ner:\n",
    "        sent = ''\n",
    "        sent_count = 0\n",
    "        token_ind_in_sent = 0\n",
    "        gold_pos = []\n",
    "        gold_wcs = []\n",
    "        y_pred_per_sent = []\n",
    "        y_true_per_sent = []\n",
    "        preds_tok_label_per_sent = []\n",
    "        n_errors = 0\n",
    "        n_words = 0\n",
    "        for index, pos_line in enumerate(fin_ner):\n",
    "            pos_line = pos_line.strip()\n",
    "            sent_id = sent_count\n",
    "            if pos_line == '':\n",
    "                if sent_count % 10 == 0:\n",
    "                    print(\"Processed {} sentences\".format(sent_count))\n",
    "                word_spans = get_spans(gold_wcs)\n",
    "                n_words += len(word_spans)\n",
    "                gold_words.append([sent[w_start:w_end] for w_start, w_end in word_spans])\n",
    "                if predictions_path is None:\n",
    "                    tokens = roberta.encode(sent)\n",
    "                    ner_preds = roberta.predict_tags('ner', tokens)\n",
    "                else:\n",
    "                    ner_preds = predictions[sent_count]\n",
    "                    assert len(ner_preds) == len(word_spans), f'{len(ner_preds)}, {len(word_spans)}'\n",
    "                for ind, (w_start, w_end) in enumerate(word_spans):\n",
    "                    for g in [gold_pos]:\n",
    "                        assert \"VOID\" not in g[w_start:w_end], \"Error! Found a VOID tag in gold labels for the word {}\".format(sent[w_start:w_end])\n",
    "                        assert len(set(g[\n",
    "                                       w_start:w_end])) == 1, \"Error! Gold labels for word {} contain more than one unique tag: {}\".format(\n",
    "                            sent[w_start:w_end], g[w_start:w_end])\n",
    "                    if predictions_path is not None:\n",
    "                        all_pred_tokens = ner_preds[ind].split('+')\n",
    "                    else:\n",
    "                        if use_prediction_spans:\n",
    "                            union_pos_pred = convert_by_pred_spans(ner_preds[w_start:w_end])\n",
    "                            all_pred_tokens = union_pos_pred.split('+')\n",
    "                        elif use_majority_vote:\n",
    "                            union_pos_pred = majority_vote(ner_preds[w_start:w_end])\n",
    "                            all_pred_tokens = union_pos_pred.split('+')\n",
    "                        else:  # prediction by first token\n",
    "                            all_pred_tokens = ner_preds[w_start].split('+')\n",
    "\n",
    "                    all_gold_tokens = convert_by_pred_spans(gold_pos[w_start:w_end]).split('+')\n",
    "\n",
    "                    if sorted(all_pred_tokens) != sorted(all_gold_tokens):\n",
    "                        n_errors += 1\n",
    "                        print(\"========================\")\n",
    "                        print(\"Wrong prediction in sentence id:\", sent_id)\n",
    "                        print(\"Sentence:\", sent)\n",
    "                        print(\"Word:\", sent[w_start:w_end])\n",
    "                        print(\"Pred labels:\", ner_preds[w_start:w_end] if predictions_path is None else ner_preds[ind])\n",
    "                        print(\"Gold labels:\", gold_pos[w_start:w_end])\n",
    "\n",
    "                        error_stats.append({\"sent_id\": sent_id,\n",
    "                                            \"sent_text\": sent,\n",
    "                                            \"word\": sent[w_start:w_end],\n",
    "                                            \"pred_pos\": ner_preds[w_start:w_end] if predictions_path is None else ner_preds[ind],\n",
    "                                            \"gold_pos\": gold_pos[w_start:w_end],\n",
    "                                            \"pred_mset\": all_pred_tokens,\n",
    "                                            \"gold_mset\": all_gold_tokens})\n",
    "\n",
    "                        print(\"Separated pred labels:\", all_pred_tokens)\n",
    "                        print(\"Separated gold labels:\", all_gold_tokens)\n",
    "\n",
    "                    y_true_per_sent.append(all_gold_tokens[0])\n",
    "                    y_pred_per_sent.append(all_pred_tokens[0])\n",
    "                    preds_tok_label_per_sent.append((sent[w_start:w_end], all_pred_tokens[0]))\n",
    "                sent = ''\n",
    "                token_ind_in_sent = 0\n",
    "                gold_pos = []\n",
    "                gold_wcs = []\n",
    "                y_true.append(y_true_per_sent)\n",
    "                y_pred.append(y_pred_per_sent)\n",
    "                preds_tok_label.append(preds_tok_label_per_sent)\n",
    "                y_true_per_sent = []\n",
    "                y_pred_per_sent = []\n",
    "                preds_tok_label_per_sent = []\n",
    "                sent_count += 1\n",
    "                continue\n",
    "\n",
    "            pos_label, token, wc = parse_file_line(pos_line, with_word_counts=True)\n",
    "            token_ind_in_sent += 1\n",
    "            sent += token\n",
    "\n",
    "            gold_pos.append(pos_label)\n",
    "            gold_wcs.append(wc)\n",
    "    \n",
    "    ner_f1 = None\n",
    "#     ner_f1 = f1_score(y_true, y_pred, average=f1_type, scheme=scheme)\n",
    "    ner_f1 = f1_score(y_true, y_pred)\n",
    "    print(classification_report(y_true, y_pred, digits=4, mode='strict', scheme=scheme))\n",
    "\n",
    "    if output_file is not None:\n",
    "        logging_str = \"============ Results ============\\n\"\n",
    "        logging_str += \"Checkpoint path: {}\\n\".format(checkpoint_path)\n",
    "        logging_str += \"Heuristics used: Majority={}, Prediction Spans={}\\n\".format(use_majority_vote,\n",
    "                                                                                    use_prediction_spans)\n",
    "        logging_str += \"{} F1 on {}: {:.4f}\\n\".format(f1_type, split, ner_f1)\n",
    "        open(output_file, 'w').write(logging_str)\n",
    "        open('{}.json'.format(output_file), 'w', encoding='utf-8').write(\n",
    "            json.dumps(error_stats, indent=4, ensure_ascii=False))\n",
    "        with open('{}_for_ner_eval.txt'.format(str(Path(checkpoint_path).parent) if predictions_path is None else str(Path(predictions_path).parent)), \n",
    "                  'w', encoding='utf-8') as f:\n",
    "            for sent_preds in preds_tok_label:\n",
    "                for tok, label in sent_preds:\n",
    "                    f.write(f'{tok} {label}\\n')\n",
    "                f.write('\\n')\n",
    "\n",
    "    return ner_f1, gold_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "_, dev_gold_words = calc_ner_f1(\n",
    "    checkpoint_path=r\"checkpoints\\punct_corrected_nemo_multi_token_base_he_from_last_lr_5e-05_bsz_32_5epochs\\checkpoint5.pt\",\n",
    "    split=\"dev\",\n",
    "    use_majority_vote=False,\n",
    "    use_prediction_spans=False,\n",
    "    use_data_from_dir=True,\n",
    "    output_file=r\"ner_roberta_base_he_nemo_token_multi_test_first_token_dev.txt\",\n",
    "    gold_data_dir=r\"finetuning_data\\ner\\data\\raw_data_he\\nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gold_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "calc_ner_f1(\n",
    "    checkpoint_path=r\"checkpoints\\roberta_base_lambda_5_oscar_3e-04\\finetune_ner\\nemo_ner_base_he_from_last_lr_1e-04_bsz_32_5epochs\\checkpoint5.pt\",\n",
    "#     checkpoint_path=r\"checkpoints\\ner_base_ar_from_last_lr_5e-05_bsz_32_5epochs\\checkpoint5.pt\",\n",
    "#     checkpoint_path=r\"checkpoints\\punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_929721_15epochs\\checkpoint_best.pt\",\n",
    "#     checkpoint_path=r\"checkpoints\\punct_corrected_nemo_token_single_base_he_from_last_lr_5e-05_bsz_32_seed_42298_15epochs\\checkpoint_best.pt\",\n",
    "#     checkpoint_path=r\"checkpoints\\punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_5023924_15epochs\\checkpoint_best.pt\",\n",
    "#     checkpoint_path=r\"checkpoints\\punct_corrected_nemo_token_single_base_he_from_last_lr_5e-05_bsz_32_seed_903443_15epochs\\checkpoint_best.pt\",\n",
    "#     checkpoint_path=r\"checkpoints\\punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_32763_15epochs\\checkpoint_best.pt\",\n",
    "#     checkpoint_path=r\"checkpoints\\punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_569021_15epochs\\checkpoint_best.pt\",\n",
    "#     checkpoint_path=r\"checkpoints\\punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_74212987_15epochs\\checkpoint_best.pt\",\n",
    "#     checkpoint_path=r\"checkpoints\\ner_base_ar_from_last_lr_3e-05_bsz_16_5epochs\\checkpoint5.pt\",\n",
    "    split=\"test\",\n",
    "    use_majority_vote=True,\n",
    "    use_prediction_spans=False,\n",
    "    use_data_from_dir=True,\n",
    "    gold_data_dir=r\"finetuning_data\\ner\\raw_data_he\\nemo\",\n",
    "    ner_format='single',\n",
    "    scheme=IOB2,\n",
    "    output_file=r'arabertv01_ner_lr_3e-05_bsz_64_5epochs_test.txt',\n",
    "    f1_type=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alephbert_pred_files = [\n",
    "#     r\"checkpoints\\alephbert_ner_nemo_token_single_lr_3e-05_bsz_16_seed_929721_15epochs\\predictions.txt\",\n",
    "#     r\"checkpoints\\alephbert_ner_nemo_token_single_lr_3e-05_bsz_16_seed_5023924_15epochs\\predictions.txt\", \n",
    "#     r\"checkpoints\\alephbert_ner_nemo_token_single_lr_3e-05_bsz_16_seed_929721_15epochs\\dev_predictions.txt\",\n",
    "#     r\"checkpoints\\alephbert_ner_nemo_token_single_lr_3e-05_bsz_16_seed_5023924_15epochs\\dev_predictions.txt\"\n",
    "]\n",
    "\n",
    "tavbert_pred_files = [\n",
    "    r\"alephbert_ner_nemo_token_single_lr_3e-05_bsz_16_seed_929721_15epochs_for_ner_eval.txt\",\n",
    "r\"alephbert_ner_nemo_token_single_lr_3e-05_bsz_16_seed_5023924_15epochs_for_ner_eval.txt\",\n",
    "    r\"punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_32763_15epochs_for_ner_eval.txt\",\n",
    "r\"punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_929721_15epochs_for_ner_eval.txt\",\n",
    "r\"punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_5023924_15epochs_for_ner_eval.txt\",\n",
    "#r\"punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_42298_15epochs_dev.txt\",\n",
    "r\"punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_32763_15epochs_for_ner_eval_dev.txt\",\n",
    "r\"punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_569021_15epochs_for_ner_eval_test.txt\",\n",
    "r\"punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_569021_15epochs_for_ner_eval_dev.txt\",\n",
    "r\"punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_5023924_15epochs_for_ner_eval_dev.txt\",\n",
    "r\"punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_929721_15epochs_for_ner_dev.txt\",\n",
    "r\"punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_74212987_15epochs_for_ner_eval.txt\",\n",
    "    r\"punct_corrected_nemo_token_single_base_he_from_last_lr_3e-05_bsz_16_seed_74212987_15epochs_for_ner_eval_dev.txt\",\n",
    "    \n",
    "]\n",
    "\n",
    "gold_path = r\"finetuning_data\\ner\\raw_data_he\\nemo\\raw_tokens\\token-single_gold_test.bmes\"\n",
    "dev_path = r\"finetuning_data\\ner\\raw_data_he\\nemo\\raw_tokens\\token-single_gold_dev.bmes\"  \n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "f1_scores = {'alephbert': {}, 'tavbert': {}}\n",
    "\n",
    "pat = re.compile(r'^.*_lr_(?P<lr>.*)_bsz_(?P<bsz>.*)_seed_(?P<seed>[0-9]+).*$')\n",
    "\n",
    "def get_predictions_from_huggingface_output(pred_file):\n",
    "    preds = []\n",
    "    with open(pred_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            preds.append(line.split(' '))\n",
    "    return preds\n",
    "\n",
    "for _ in ['dev', 'test']:\n",
    "#     for aleph_p in alephbert_pred_files:\n",
    "#         preds = get_predictions_from_huggingface_output(aleph_p)\n",
    "#         assert len(gold_words) == len(preds)\n",
    "#         assert [len(g) == len(p) for g, p in zip(gold_words, preds)]\n",
    "#         print(aleph_p)\n",
    "#         pred_path = os.path.basename(str(Path(aleph_p).parent) + '_for_ner_eval_dev.txt')\n",
    "#         with open(pred_path, 'w', encoding='utf-8') as fobj:\n",
    "#             for i in range(len(preds)):\n",
    "#                 for w, p in list(zip(*list(zip(gold_words, preds))[i])):\n",
    "#                     fobj.write(f'{w} {p}\\n')\n",
    "#                 fobj.write('\\n')\n",
    "\n",
    "#         seed = int(pat.match(aleph_p).groupdict()['seed'])\n",
    "#         _, _, f1_s = ne_evaluate_mentions.evaluate_files(gold_path=dev_path, pred_path=pred_path, verbose=True)\n",
    "#         f1_scores['alephbert'][seed] = {}\n",
    "#         f1_scores['alephbert'][seed][split] = 100 * f1_s\n",
    "\n",
    "    for tav_p in tavbert_pred_files:\n",
    "    #     preds = get_predictions_from_huggingface_output(tav_p)\n",
    "    #     assert len(gold_words) == len(preds), f\"{len(gold_words)}, {len(preds)}\"\n",
    "    #     assert [len(g) == len(p) for g, p in zip(gold_words, preds)]\n",
    "#         if (split == 'dev' and not tav_p.endswith(\"dev.txt\")) or (split == 'test' and tav_p.endswith(\"dev.txt\")):\n",
    "#             continue\n",
    "        print(tav_p)\n",
    "        assert pat.match(tav_p).groupdict()['lr'] == '3e-05'\n",
    "        assert int(pat.match(tav_p).groupdict()['bsz']) == 16\n",
    "        seed = int(pat.match(tav_p).groupdict()['seed'])\n",
    "        _, _, f1_s_a = ne_evaluate_mentions.evaluate_files(gold_path=dev_path, pred_path=tav_p, verbose=False)\n",
    "        _, _, f1_s_b = ne_evaluate_mentions.evaluate_files(gold_path=gold_path, pred_path=tav_p, verbose=False)\n",
    "        print(\"dev:\", f1_s_a, \"test:\", f1_s_b)\n",
    "        f1_s = f1_s_a if f1_s_a != -1 else f1_s_b\n",
    "        split = 'dev' if f1_s_a != -1 else 'test'\n",
    "        if seed not in f1_scores['tavbert']:\n",
    "            f1_scores['tavbert'][seed] = {}\n",
    "        f1_scores['tavbert'][seed][split] = 100 * f1_s\n",
    "        pprint.pprint(f1_scores)\n",
    "\n",
    "print(\"===================================================\")\n",
    "pprint.pprint(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# pred_file = r\"checkpoints\\alephbert_ner_nemo_token_multi_lr_5e-05_bsz_64_6epochs\\test_predictions.txt\"\n",
    "# pred_file = r\"checkpoints\\alephbert_ner_nemo_token_single_lr_3e-05_bsz_64_6epochs\\test_predictions.txt\"\n",
    "# pred_file = r\"checkpoints\\alephbert_ner_nemo_token_single_lr_3e-05_bsz_64_5epochs\\predictions.txt\"\n",
    "# pred_file = r\"checkpoints\\alephbert_ner_nemo_token_single_lr_3e-05_bsz_16_seed_929721_15epochs\\predictions.txt\"\n",
    "pred_file = r\"checkpoints\\alephbert_ner_nemo_token_single_lr_3e-05_bsz_16_seed_5023924_15epochs\\predictions.txt\"\n",
    "#     \n",
    "preds = get_predictions_from_huggingface_output(pred_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "from NEMO import ne_evaluate_mentions\n",
    "\n",
    "# _, gold_words = calc_ner_f1(\n",
    "# #     checkpoint_path=r\"checkpoints\\roberta_base_lambda_5_oscar_3e-04\\finetune_ner\\nemo_ner_base_he_from_last_lr_1e-04_bsz_32_5epochs\\checkpoint5.pt\",\n",
    "#     checkpoint_path=r\"checkpoints\\punct_corrected_nemo_multi_token_base_he_from_last_lr_5e-05_bsz_32_5epochs\\checkpoint5.pt\",\n",
    "#     split=\"test\",\n",
    "#     use_majority_vote=False,\n",
    "#     use_prediction_spans=False,\n",
    "#     use_data_from_dir=True,\n",
    "#     output_file=r\"ner_roberta_base_he_nemo_token_multi_test_first_token.txt\",\n",
    "#     gold_data_dir=r\"finetuning_data\\ner\\raw_data_he\\nemo\")\n",
    "gold_path = r\"finetuning_data\\ner\\raw_data_he\\nemo\\raw_tokens\\token-single_gold_test.bmes\"\n",
    "\n",
    "# #### CHANGE THIS! ####\n",
    "# pred_files = [r\"checkpoints\\alephbert_ner_nemo_token_single_lr_3e-05_bsz_16_seed_929721_15epochs\\predictions.txt\",\n",
    "#               r\"checkpoints\\alephbert_ner_nemo_token_single_lr_3e-05_bsz_16_seed_5023924_15epochs\\predictions.txt\"]\n",
    "\n",
    "pred_files = [r\"checkpoints\\alephbert_ner_nemo_token_single_lr_3e-05_bsz_16_seed_5023924_15epochs\\predictions.txt\"]\n",
    "def get_predictions_from_huggingface_output(pred_file):\n",
    "    preds = []\n",
    "    with open(pred_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            preds.append(line.split(' '))\n",
    "    return preds\n",
    "\n",
    "pred_paths = []\n",
    "for pred_file in pred_files:\n",
    "    preds = get_predictions_from_huggingface_output(pred_file)\n",
    "\n",
    "    assert len(gold_words) == len(preds)\n",
    "    assert [len(g) == len(p) for g, p in zip(gold_words, preds)]\n",
    "\n",
    "\n",
    "    pred_path = os.path.basename(str(Path(pred_file).parent))\n",
    "    pred_paths.append(pred_path)\n",
    "    with open(pred_path, 'w', encoding='utf-8') as fobj:\n",
    "        for i in range(len(preds)):\n",
    "            for w, p in list(zip(*list(zip(gold_words, preds))[i])):\n",
    "                fobj.write(f'{w} {p}\\n')\n",
    "            fobj.write('\\n')\n",
    "\n",
    "f1_scores = []\n",
    "print(pred_path)\n",
    "for pred_path in [\n",
    "    r\"punct_corrected_nemo_token_single_base_he_from_last_lr_5e-05_bsz_32_seed_42298_15epochs_for_ner_eval.txt\",\n",
    "    r\"punct_corrected_nemo_token_single_base_he_from_last_lr_5e-05_bsz_32_seed_903443_15epochs_for_ner_eval.txt\"\n",
    "]:\n",
    "    print(pred_path)\n",
    "    _, _, f1_s = ne_evaluate_mentions.evaluate_files(gold_path=gold_path, pred_path=pred_path, verbose=True)\n",
    "    f1_scores.append(f1_s)\n",
    "\n",
    "f1_scores = np.array(f1_scores)\n",
    "print(f1_scores)\n",
    "print(100 * np.mean(f1_scores), '+-', np.std(100 * f1_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

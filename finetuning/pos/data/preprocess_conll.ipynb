{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pprint\n",
    "import csv\n",
    "import pandas as pd\n",
    "import pyconll\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sacremoses import MosesDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARABIC_SPECIAL_CHARS = ['ً', 'ّ', 'ٍ', 'ٌ', 'ـ']\n",
    "\n",
    "def join_labels(labels, feat):\n",
    "    if feat != 'pos':\n",
    "        joint_labels = '+'.join([l for l in labels if l != 'X'])\n",
    "        if len(joint_labels) == 0:\n",
    "            joint_labels = 'X'\n",
    "    else:\n",
    "        joint_labels = '+'.join(labels)\n",
    "    return joint_labels\n",
    "\n",
    "\n",
    "def dump_labels_to_file(sent_ids, sents_labels, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for sent_id, sent_labels in zip(sent_ids, sents_labels):\n",
    "            f.write(f'{sent_id}\\n')\n",
    "            for tok, label in sent_labels:\n",
    "                f.write(f'{tok} {label}\\n')\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "def align_labels_to_morphemes(orig_tokens, morpheme_toks, morpheme_labels, feat='pos'):\n",
    "    \"\"\"Applies alignment heuristics to map the morpheme labels to the given list of tokens (characters). \n",
    "    Example:\n",
    "    >>> align_labels_to_morphemes(['ו', 'כ', 'ש', 'מ', 'ד', 'ב', 'ר', 'י', 'ה', 'ם'],\n",
    "                                  ['ו', 'כש', 'מ', 'דברים', 'של', 'הם'],\n",
    "                                  ['CCONJ', 'SCONJ', 'ADP', 'NOUN', 'ADP', 'PRON'],\n",
    "                                  feat='pos')\n",
    "    >>> [('ו', 'CCONJ'),\n",
    "         ('כ', 'SCONJ'),\n",
    "         ('ש', 'SCONJ'),\n",
    "         ('מ', 'ADP'),\n",
    "         ('ד', 'NOUN+ADP+PRON'),\n",
    "         ('ב', 'NOUN+ADP+PRON'),\n",
    "         ('ר', 'NOUN+ADP+PRON'),\n",
    "         ('י', 'NOUN+ADP+PRON'),\n",
    "         ('ה', 'NOUN+ADP+PRON'),\n",
    "         ('ם', 'NOUN+ADP+PRON')]\n",
    "    \"\"\"\n",
    "    curr = 0\n",
    "    orig_curr = 0\n",
    "    labels_full = []\n",
    "    tokens_full = []\n",
    "    joint_labels = None\n",
    "    prev = None\n",
    "    \n",
    "    if orig_tokens[-1] in ARABIC_SPECIAL_CHARS and not morpheme_toks[-1].endswith(orig_tokens[-1]):\n",
    "        morpheme_toks[-1] += orig_tokens[-1]\n",
    "        \n",
    "    while curr < len(morpheme_toks):\n",
    "        if prev is not None:\n",
    "            labels_full.extend([morpheme_labels[prev[1]] for ch in morpheme_toks[prev[1]]])\n",
    "            tokens_full.extend([ch for ch in morpheme_toks[prev[1]]])\n",
    "            prev = None\n",
    "        \n",
    "        # Hebrew covert morphemes - join the respective lables into a multitag\n",
    "        if ''.join(orig_tokens[orig_curr:]) != ''.join(morpheme_toks[curr:]) and \\\n",
    "            (morpheme_toks[curr:curr + 2] == ['ב', 'ה'] \\\n",
    "                or morpheme_toks[curr:curr + 2] == ['ל', 'ה'] \\\n",
    "                or morpheme_toks[curr:curr + 2] == ['כ', 'ה']):\n",
    "            tokens_full.append(morpheme_toks[curr])       \n",
    "            joint_labels = join_labels(morpheme_labels[curr:curr + 2], feat)\n",
    "            labels_full.append(joint_labels)\n",
    "            orig_curr += 1\n",
    "            curr += 2\n",
    "            continue\n",
    "\n",
    "        if ''.join(orig_tokens[orig_curr:]) != ''.join(morpheme_toks[curr:]):\n",
    "            if ''.join(orig_tokens[orig_curr:orig_curr + len(morpheme_toks[curr])]) == ''.join(orig_tokens[orig_curr:]):\n",
    "                p = prev if prev is not None else (orig_curr, curr)\n",
    "                joint_labels = join_labels(morpheme_labels[p[1]:], feat)\n",
    "                labels_full.extend([joint_labels for ch in orig_tokens[p[0]:]])\n",
    "                tokens_full.extend([ch for ch in orig_tokens[p[0]:]])\n",
    "                break\n",
    "            if ''.join(orig_tokens[orig_curr:orig_curr + len(morpheme_toks[curr])]) == morpheme_toks[curr]:\n",
    "                prev = (orig_curr, curr)\n",
    "            else:\n",
    "                p = prev if prev is not None else (orig_curr, curr)\n",
    "                joint_labels = join_labels(morpheme_labels[p[1]:], feat)\n",
    "                labels_full.extend([joint_labels for ch in orig_tokens[p[0]:]])\n",
    "                tokens_full.extend([ch for ch in orig_tokens[p[0]:]])\n",
    "                break\n",
    "        else:\n",
    "            if ''.join(orig_tokens[orig_curr:orig_curr + len(morpheme_toks[curr])]) == morpheme_toks[curr]:\n",
    "                labels_full.extend([morpheme_labels[curr] for ch in morpheme_toks[curr]])\n",
    "                tokens_full.extend([ch for ch in morpheme_toks[curr]])\n",
    "            else:\n",
    "                joint_labels = join_labels(morpheme_labels[curr:], feat)\n",
    "                labels_full.extend([joint_labels for ch in orig_tokens[curr:]])\n",
    "                tokens_full.extend([ch for ch in orig_tokens[orig_curr:]])\n",
    "                break\n",
    "        orig_curr += len(morpheme_toks[curr])\n",
    "        curr += 1\n",
    "    return list(zip(tokens_full, labels_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ל', 'ADP'),\n",
      " ('ה', 'DET'),\n",
      " ('פ', 'NOUN'),\n",
      " ('ו', 'NOUN'),\n",
      " ('ע', 'NOUN'),\n",
      " ('ל', 'NOUN')]\n",
      "[('و', 'CCONJ'),\n",
      " ('م', 'ADJ'),\n",
      " ('ت', 'ADJ'),\n",
      " ('س', 'ADJ'),\n",
      " ('ا', 'ADJ'),\n",
      " ('ر', 'ADJ'),\n",
      " ('ع', 'ADJ'),\n",
      " ('ا', 'ADJ'),\n",
      " ('ً', 'ADJ')]\n"
     ]
    }
   ],
   "source": [
    "### Hebrew\n",
    "pprint.pprint(align_labels_to_morphemes(['ל', 'ה', 'פ', 'ו', 'ע', 'ל'],\n",
    "                                  ['ל', 'ה', 'פועל'],\n",
    "                                  ['ADP', 'DET', 'NOUN'],\n",
    "                                  feat='pos'))\n",
    "\n",
    "### Arabic\n",
    "pprint.pprint(align_labels_to_morphemes(['و', 'م', 'ت', 'س', 'ا', 'ر', 'ع', 'ا', 'ً'],\n",
    "                                        ['و', 'متسارعا'],\n",
    "                                        ['CCONJ', 'ADJ']))\n",
    "\n",
    "# ### Turkish\n",
    "# pprint.pprint(align_labels_to_morphemes(['t', 'u', 't', 's', 'a', 'ğ', 'ı', 'm'],\n",
    "#                                         ['tutsak', 'ım'],\n",
    "#                                         ['ADJ', 'AUX']))\n",
    "\n",
    "# pprint.pprint(align_labels_to_morphemes(['s', 'a', 'y', 'f', 'a', 'l', 'ı', 'k'],\n",
    "#                                         ['sayfa', 'lık'],\n",
    "#                                         ['NOUN', 'ADP']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_pattern = re.compile(\"(?P<start>[0-9]+)-(?P<end>[0-9]+)\")\n",
    "\n",
    "VOID_TAG = \"VOID\"\n",
    "\n",
    "\n",
    "def is_space_after_token(token):\n",
    "    return token.misc.get('SpaceAfter') != {'No'}\n",
    "\n",
    "\n",
    "def tokenize_chars(text):\n",
    "    return [ch for ch in text]\n",
    "\n",
    "def get_sentence_toks_and_morphs(sentence, feats):\n",
    "    start, end = -1, -1\n",
    "    toks = []\n",
    "    tok_inds = []\n",
    "    morpheme_toks = []\n",
    "    spaces = []\n",
    "    morpheme_labels = defaultdict(list)\n",
    "    for token in sentence:\n",
    "        if \".\" in token.id:\n",
    "            # Arabic parataxis (e.g. 27.1)\n",
    "            continue\n",
    "        try:\n",
    "            token_id = int(token.id)\n",
    "        except ValueError:\n",
    "            # This is a span\n",
    "            start, end = [int(m) for m in span_pattern.match(token.id).groups()]\n",
    "            space_after = is_space_after_token(token)\n",
    "            spaces.append(space_after)\n",
    "            orig_token_chars = tokenize_chars(token.form)\n",
    "            tok_inds.append(list(range(start - 1, end)))\n",
    "            toks.append(token.form)\n",
    "            continue\n",
    "            \n",
    "        if token_id not in range(start, end + 1):\n",
    "            toks.append(token.form)\n",
    "            tok_inds.append([token_id - 1])\n",
    "            space_after = is_space_after_token(token)\n",
    "            spaces.append(space_after)\n",
    "        morpheme_toks.append(token.form)\n",
    "        morpheme_labels['pos'].append(token.upos)\n",
    "        for f in feats:\n",
    "            tok_f = '+'.join(sorted(list(token.feats.get(f, {'X'}))))\n",
    "            morpheme_labels[f].append(tok_f)\n",
    "    spaces[-1] = False\n",
    "    return toks, tok_inds, morpheme_toks, morpheme_labels, spaces\n",
    "\n",
    "\n",
    "def preprocess_ud(output_dir, conll_file_path, output_format='segmented', json_char_format=True, feats=[]):\n",
    "    sents = {'train': {}, 'dev': {}, 'test': {}}\n",
    "    json_sents = {'train': {}, 'dev': {}, 'test': {}}\n",
    "    sent_ids = {}\n",
    "    for sp in sents:\n",
    "        for f in feats + ['pos']:\n",
    "            sents[sp][f] = []\n",
    "            \n",
    "    for sp in sents:\n",
    "        json_sents[sp] = []\n",
    "    \n",
    "    for split in ['train', 'dev', 'test']:\n",
    "        conll_obj = pyconll.load_from_file(conll_file_path.format(split))\n",
    "        sent_ids[split] = []\n",
    "        for i, sentence in enumerate(conll_obj):\n",
    "            sent_id = sentence.id\n",
    "            sent_ids[split].append(sent_id)\n",
    "            sent_text = sentence.text\n",
    "            tags = defaultdict(list)\n",
    "            json_tags = defaultdict(list)\n",
    "            toks, tok_inds, morphs, morpheme_labels, spaces = get_sentence_toks_and_morphs(sentence, feats)\n",
    "            assert len(set([len(morpheme_labels[f]) for f in feats + ['pos']])) == 1, \\\n",
    "                                        (sent_id, [morpheme_labels[f] for f in feats + ['pos']])\n",
    "            for f in feats + ['pos']:\n",
    "                for ind, full_tok, is_space_after in zip(tok_inds, toks, spaces):\n",
    "                    if len(ind) == 1:\n",
    "                        tok_chars = [c for c in full_tok]\n",
    "                        labels = [morpheme_labels[f][ind[0]] for _ in full_tok]\n",
    "                        tags[f].extend(list(zip(tok_chars, labels)))\n",
    "                        if output_format == 'multitag':\n",
    "                            if json_char_format:\n",
    "                                json_tags[f].extend(list(zip(tok_chars, labels)))\n",
    "                            else:\n",
    "                                json_tags[f].append((full_tok, morpheme_labels[f][ind[0]]))\n",
    "                    else:\n",
    "                        if output_format == 'multitag':\n",
    "                            joint_multitag = '+'.join([morpheme_labels[f][i] for i in ind])\n",
    "                            if f != 'pos':\n",
    "                                all_feats = [morpheme_labels[f][i] for i in ind if morpheme_labels[f][i] != 'X']\n",
    "                                joint_multitag = 'X' if len(all_feats) == 0 else '+'.join(all_feats)\n",
    "                            tags[f].extend(list(zip([c for c in full_tok], \n",
    "                                                    [joint_multitag for _ in full_tok])))\n",
    "                            if json_char_format:\n",
    "                                json_tags[f].extend(list(zip([c for c in full_tok], \n",
    "                                                    [joint_multitag for _ in full_tok])))\n",
    "                            else:\n",
    "                                json_tags[f].append((full_tok, joint_multitag))\n",
    "                        else: # segmented format\n",
    "                            tags[f].extend(align_labels_to_morphemes([c for c in full_tok], \n",
    "                                                                     [morphs[i].replace('_', '') for i in ind], \n",
    "                                                                     [morpheme_labels[f][i] for i in ind], f))\n",
    "                    if is_space_after:\n",
    "                        tags[f].append((' ', 'VOID'))\n",
    "                        if output_format == 'multitag' and json_char_format:\n",
    "                            json_tags[f].append((' ', 'VOID'))\n",
    "\n",
    "            if len(set([len(tags[f]) for f in feats + ['pos']] + [len(sent_text)])) != 1:\n",
    "                print(f\"======= Problem in sentence id {sent_id}:\")\n",
    "                print(sent_text)\n",
    "                print([len(tags[f]) for f in feats + ['pos']] + [len(sent_text)])\n",
    "            \n",
    "            sent_dict = {}\n",
    "            for f in feats + ['pos']:\n",
    "                sents[split][f].append(tags[f])\n",
    "                if f == 'pos':\n",
    "                    sent_dict['tokens'] = [t[0] for t in json_tags[f]] if output_format == 'multitag' else [t[0] for t in tags[f]]\n",
    "                if output_format == 'multitag':\n",
    "                    sent_dict[f'{f.lower()}_tags'] = [t[1] for t in json_tags[f]]\n",
    "                else:\n",
    "                    sent_dict[f'{f.lower()}_tags'] = [t[1] for t in tags[f]]\n",
    "            json_sents[split].append(sent_dict)\n",
    "            \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    for sp, sents_labels in sents.items():\n",
    "        for f, sent_labels in sents_labels.items():\n",
    "            dump_labels_to_file(sent_ids[sp], sent_labels, \n",
    "                                os.path.join(output_dir, \n",
    "                                             f'{f.lower()}_{sp}_{output_format}_ud.txt'))\n",
    "    \n",
    "    for sp, sent_labels in json_sents.items():\n",
    "        json_format = 'chars' if json_char_format else 'words'\n",
    "        filename = f'{sp}_{output_format}_{json_format}_ud.json'\n",
    "        open(os.path.join(output_dir, filename), 'w',\n",
    "             encoding='utf-8').write(\n",
    "            json.dumps({'format': output_format, 'data': sent_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ud_paths = {'ar': \"raw_data_ar/UD_Arabic-PADT/ar_padt-ud-{}.conllu\",\n",
    "            'he': \"raw_data_he/UD_Hebrew-HTB/he_htb-ud-{}.conllu\",\n",
    "            'tr': \"raw_data_tr/UD_Turkish-IMST/tr_imst-ud-{}.conllu\"}\n",
    "\n",
    "feats_per_lang = {'ar': ['Abbr',\n",
    "                         'AdpType',\n",
    "                         'Aspect',\n",
    "                         'Case',\n",
    "                         'ConjType',\n",
    "                         'Definite',\n",
    "                         'Foreign',\n",
    "                         'Gender',\n",
    "                         'Mood',\n",
    "                         'Number',\n",
    "                         'NumForm',\n",
    "                         'NumValue',\n",
    "                         'Person',\n",
    "                         'Polarity',\n",
    "                         'PronType',\n",
    "                         'VerbForm',\n",
    "                         'Voice'],\n",
    "                  'he': ['Abbr',\n",
    "                         'Case',\n",
    "                         'Definite',\n",
    "                         'Gender',\n",
    "                         'HebBinyan',\n",
    "                         'HebExistential',\n",
    "                         'Mood',\n",
    "                         'Number',\n",
    "                         'Person',\n",
    "                         'Polarity',\n",
    "                         'Prefix',\n",
    "                         'PronType',\n",
    "                         'Reflex',\n",
    "                         'Tense',\n",
    "                         'VerbForm',\n",
    "                         'VerbType',\n",
    "                         'Voice'],\n",
    "                  'tr': ['Abbr',\n",
    "                         'Aspect',\n",
    "                         'Case',\n",
    "                         'Definite',\n",
    "                         'Echo',\n",
    "                         'Evident',\n",
    "                         'Mood',\n",
    "                         'Number',\n",
    "                         'Number[psor]',\n",
    "                         'NumType',\n",
    "                         'Person',\n",
    "                         'Person[psor]',\n",
    "                         'Polarity',\n",
    "                         'Polite',\n",
    "                         'PronType',\n",
    "                         'Reflex',\n",
    "                         'Tense',\n",
    "                         'VerbForm',\n",
    "                         'Voice']}\n",
    "\n",
    "# for lang in ['ar', 'he', 'tr']:\n",
    "for lang in ['he', 'tr']:\n",
    "    for output_format, json_char_format in [('segmented', True), ('multitag', True), ('multitag', False)]:\n",
    "        preprocess_ud(output_dir=os.path.join(str(Path(ud_paths[lang]).parent), \n",
    "                                              f'{output_format}_{\"chars\" if json_char_format else \"words\"}'),\n",
    "                      conll_file_path=ud_paths[lang],\n",
    "                      output_format=output_format,\n",
    "                      json_char_format=json_char_format,\n",
    "                      feats=feats_per_lang[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['global_global_sent_id', 'global_TOKEN', \n",
    "        'sp_orig_sent', 'sp_token_str', 'sp_forms', 'sp_morph_ids', \n",
    "        'sp_count', 'ud_orig_sent', 'ud_token_str', \n",
    "        'ud_forms', 'ud_morph_ids', 'ud_count', \n",
    "        'fixed_token_str', 'fixed_sp_forms', 'fixed_ud_forms', 'comment']\n",
    "\n",
    "conll_obj = pyconll.load_from_file(r\"spmrl_fixed.conllu\")\n",
    "conll_data = defaultdict(list)\n",
    "with open(r\"token_morpheme_alignment_spmrl_ud_with_fixes.csv\", 'r', encoding='utf-8') as infile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    for row in reader:\n",
    "        conll_data[row['global_global_sent_id']].append(row)\n",
    "\n",
    "def preprocess_spmrl(output_dir):\n",
    "    detok = MosesDetokenizer(lang='he')\n",
    "    sents = {'train': {}, 'dev': {}, 'test': {}}\n",
    "    feats = ['gen', 'num', 'per', 'polar', 'tense', 'HebBinyan', 'suf_num', 'suf_per',\n",
    "                                                  'suf_gen']\n",
    "\n",
    "    for sp in sents:\n",
    "        for f in feats + ['pos']:\n",
    "            sents[sp][f] = []\n",
    "\n",
    "    output_format = 'multitag'\n",
    "\n",
    "    for sentence in conll_obj:\n",
    "        sent_id = int(sentence.id)\n",
    "        split = sentence._meta['set']\n",
    "        if sent_id % 100 == 0:\n",
    "            print(sent_id)\n",
    "        tags = defaultdict(list)\n",
    "        morpheme_labels = defaultdict(list)\n",
    "\n",
    "        morpheme_labels['pos'] = [token.upos for token in sentence if hasattr(token, 'upos') and token.upos is not None]\n",
    "        for f in feats:\n",
    "            toks_f = ['+'.join(sorted(list(token.feats.get(f, {'X'})))) for token in sentence if token.upos is not None]\n",
    "            morpheme_labels[f] = toks_f\n",
    "        assert len(set([len(morpheme_labels[f]) for f in feats + ['pos']])) == 1, (sent_id, [morpheme_labels[f] for f in feats + ['pos']])\n",
    "\n",
    "        sent_rows = conll_data[str(sent_id)]\n",
    "        morphs = [r['fixed_sp_forms'] if r['fixed_sp_forms'] != '' else r['sp_forms'] for r in sent_rows]\n",
    "        toks = [r['fixed_token_str'] if r['fixed_token_str'] != '' else r['sp_token_str'] for r in sent_rows]\n",
    "        sent_text = detok.detokenize(toks)\n",
    "        tok_inds = [eval(r['sp_morph_ids']) for r in sent_rows]\n",
    "        tok_inds = [[int(y) - 1 for y in t] for t in tok_inds]\n",
    "        for f in feats + ['pos']:\n",
    "            for ind, full_tok, tok_morphs in zip(tok_inds, toks, morphs):\n",
    "                # manual fix \n",
    "                if sent_id == 4009 and tok_morphs == 'ב 19.9':\n",
    "                    tok_morphs = 'ב 19.90'\n",
    "                    print(\"Manual Fix!!!\")\n",
    "                if len(ind) == 1:\n",
    "                    tok_chars = [c for c in full_tok]\n",
    "                    labels = [morpheme_labels[f][ind[0]] for _ in full_tok]\n",
    "                    tags[f].extend(list(zip(tok_chars, labels)))\n",
    "                else:\n",
    "                    if output_format == 'multitag':\n",
    "                        joint_multitag = '+'.join([morpheme_labels[f][i] for i in ind])\n",
    "                        if f != 'pos' and all([morpheme_labels[f][i] == 'X' for i in ind]):\n",
    "                            joint_multitag = 'X'\n",
    "                        tags[f].extend(list(zip([c for c in full_tok], \n",
    "                                                [joint_multitag for _ in full_tok])))\n",
    "                    else: # segmented format\n",
    "                        tags[f].extend(align_labels_to_morphemes([c for c in full_tok], tok_morphs.split(), [morpheme_labels[f][i] for i in ind], f))\n",
    "\n",
    "            for ws_ind in [i.start() for i in re.finditer(' ', sent_text)]:\n",
    "                tags[f].insert(ws_ind, (' ', 'VOID'))\n",
    "        if len(set([len(tags[f]) for f in feats + ['pos']] + [len(sent_text)])) != 1:\n",
    "            print(f\"======= Problem in sentence id {sent_id}:\", sent_id)\n",
    "            print(sent_text)\n",
    "            pprint.pprint(tags['pos'])\n",
    "            print([len(tags[f]) for f in feats + ['pos']] + [len(sent_text)])\n",
    "\n",
    "        for f in feats + ['pos']:\n",
    "            sents[split][f].append(tags[f])\n",
    "\n",
    "    for sp, sents_labels in sents.items():\n",
    "        for f, sent_labels in sents_labels.items():\n",
    "            dump_labels_to_file(sent_labels, \n",
    "                                os.path.join(output_dir, \n",
    "                                             f'{f}_{sp}_{output_format}_spmrl.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_spmrl(r'preprocessed_ud_he')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

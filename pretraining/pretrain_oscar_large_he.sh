fairseq-train --fp16 data/oscar \
  --task masked_lm \
  --criterion masked_lm \
  --arch roberta_large \
  --tokens-per-sample 1024 \
  --max-tokens 1048 \
  --optimizer adam \
  --adam-betas '(0.9,0.98)' \
  --adam-eps 1e-6 \
  --clip-norm 0.0 \
  --lr-scheduler polynomial_decay \
  --lr 9e-05 \
  --total-num-update 125000 \
  --dropout 0.1 \
  --attention-dropout 0.1 \
  --weight-decay 0.01 \
  --update-freq 256 \
  --max-update 125000 \
  --log-format simple \
  --log-interval 1 \
  --skip-invalid-size-inputs-valid-test \
  --poisson-lambda 5 \
  --shorten-method truncate \
  --sample-break-mode complete_doc \
  --num-workers 2 \
  --warmup-updates 1000 \
  --save-dir checkpoints/roberta_large_lambda_5_oscar \
  --validate-interval-updates 1000 \
  --encoder-normalize-before \
  --restore-file checkpoints/roberta_large_lambda_5_oscar/checkpoint5.pt
